---
title: "Amazon Reviews Sentiment Analysis"
author: "Mirza Naseh Ahmad"
date: "3/8/2021"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


#initialization

if(!"pacman" %in% installed.packages()[,"Package"]) install.packages("pacman")
pacman::p_load(BiocManager, plyr, dplyr, readr, ggplot2 ,stringr, syuzhet, RColorBrewer,wordcloud, NLP, tm, SnowballC, knitr, tidytext, tidyr,RSentiment, DT, sqldf, tidyverse, text2vec, fastTextR, tokenizers, caTools, class, rvest, caret, quanteda, doSNOW, e1071, obliqueRF, lattice,skimr)

options(stringsASFacgtors = FALSE)
library(quanteda)
library(dplyr,tidyverse)
library(tidyverse)
library(text2vec)
library(SnowballC)
library(tidytext)
library(stringr)
library(stopwords)
library(tokenizers)
require(dplyr)
require(data.table)
require(caTools)
library(fastTextR)
library(ggplot2)
library(doSNOW)
library(e1071)
library(obliqueRF)
library(caret,lattice)
library(skimr)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r Automotive Reviews}
#setting the working directory
setwd('E:\\Ryerson\\CIND820\\Code')
#Loading the Data

#dowloading the data from the source
#fn <- "amazon_reviews_us_Automotive_v1_00.tsv.gz"

#if ( !file.exists(fn) ) {
# download.file("https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Automotive_v1_00.tsv.gz",
#               fn)
#  untar(fn)
#}
```

```{r Data and EDA}

datacomplete<-as.data.frame(fread('amazon_reviews_us_Automotive_v1_00.tsv'),stringsAsFactors = FALSE)
sampledata<-summary(datacomplete)
?fread

skimmed <- skim_to_wide(datacomplete)
skimmed[, c(1:5, 9:11, 13, 15:16)]


vine<-as.factor(datacomplete$vine)
summary(vine)

#Marketplace where the review was written.
marketplace<- as.factor(datacomplete$marketplace)
summary(marketplace)

#total Unique Customers
uniquecustomers <- unique(datacomplete$customer_id)
length(uniquecustomers)

#total Unique products reviewed
uniqueproducts <- unique(datacomplete$product_id)
length(uniqueproducts)

#Textlength 
datacomplete$reviewlength <- nchar(datacomplete$review_body)

summary(datacomplete$reviewlength)

#Highest rated products.
top_rated_products <- datacomplete %>%
  group_by(product_id) %>% 
  summarize(count_votes = n()) %>% 
  arrange(desc(count_votes))

top_rated_products1 <- top_rated_products[top_rated_products$count_votes > 10 ,]
summary(top_rated_products1)
head(top_rated_products1)

data<- datacomplete[datacomplete$product_id %in% top_rated_products1$product_id ,]

str(data)

#Star Rating
srating<-datacomplete$star_rating
hist(srating)
prop.table(table(srating))



#Data Prep
datacomplete$reviewlength <- nchar(datacomplete$review_body)

summary(datacomplete$reviewlength)
str(datacomplete)

#removing "NA" values

datacomplete<-na.omit(datacomplete)

skimmed <- skim_to_wide(datacomplete)
skimmed[, c(1:5, 9:11, 13, 15:16)]

#removing zero text values
zerotext<-datacomplete[datacomplete$reviewlength == 0 ,]
zerotext
datacomplete<-datacomplete[datacomplete$reviewlength != 0,]



#filtering out non verified purchases
vpcount = table(datacomplete$verified_purchase)
vpcount = as.data.frame(vpcount)
names(vpcount)[1] = 'Verified purchase'
vpcount


datavp<-datacomplete[datacomplete$verified_purchase != 'N' & datacomplete$product_id %in% top_rated_products1$product_id,]

table(datavp$verified_purchase)
summary(datavp$reviewlength)
glimpse(datavp)

#Stratified Sampling
set.seed(1000)
options(stringsASFacgtors = FALSE)
sr1<- filter(datavp, star_rating == 1)
sr2<- filter(datavp, star_rating == 2)
sr3<- filter(datavp, star_rating == 3)
sr4<- filter(datavp, star_rating == 4)
sr5<- filter(datavp, star_rating == 5)

sampledata1<- sample_n(sr1,74829 , replace = FALSE)
sampledata2<- sample_n(sr2,74829 , replace = FALSE)
sampledata3<- sample_n(sr3,74829 , replace = FALSE)
sampledata4<- sample_n(sr4,74829 , replace = FALSE)
sampledata5<- sample_n(sr5,74829 , replace = FALSE)

sampledata <- rbind(sampledata1, sampledata2, sampledata3, sampledata4, sampledata5)
sampledata <- data.table(rating = sampledata$star_rating ,review = sampledata$review_body , reviewlength = sampledata$reviewlength)

data <- data.table(rating = datavp$star_rating ,review = datavp$review_body , reviewlength = datavp$reviewlength)

set.seed(10)
datareduction <- sample(1:nrow(data), 0.01 * nrow(data))
data<- data[datareduction, ]

sampledatareduction <- sample(1:nrow(sampledata), 0.05 * nrow(sampledata))
sampledata<-sampledata[sampledatareduction, ]

glimpse(data)
glimpse(sampledata)


round(100*prop.table(table(data$rating)), digits = 2)
SRFactor <- as.factor(data$rating)
barplot(round(100*prop.table(table(data$rating)), digits = 2), xlab = "Star Rating", ylab = "% Frequency", main = "% Data By Star Rating Original Data", col = c(brewer.pal(9,"YlGnBu")))

round(100*prop.table(table(sampledata$rating)), digits = 2)
SRFactor <- as.factor(sampledata$rating)
barplot(round(100*prop.table(table(sampledata$rating)), digits = 2), xlab = "Star Rating", ylab = "% Frequency", main = "% Data By Star Rating Stratified Data", col = c(brewer.pal(9,"YlGnBu")))

glimpse(data)
glimpse(sampledata)




#Splitting the data into training and test set (70/30 split)

set.seed(1002)
indexes<- createDataPartition(data$rating, times = 1 ,p = 0.7, list = FALSE)
train<-data[indexes,]
test <- data[-indexes,]
train <- data.table(rating = train$rating ,review = train$review , reviewlength = train$reviewlength)
test <- data.table(rating = test$rating ,review = test$review , reviewlength = test$reviewlength)

glimpse(train)
glimpse(test)

set.seed(1001)
indexes1<- createDataPartition(sampledata$rating, times = 1 ,p = 0.7, list = FALSE)
train1 <- sampledata[indexes1,]
test1 <- sampledata[-indexes1,]

glimpse(train1)
glimpse(test1)

#

sampledatay3 <-train[train$rating == 3, ]
sampledatay5 <-train[train$rating > 3 ,]
sampledatay1 <-train[train$rating <3 ,]
glimpse(sampledatay3)

#stratified sample
sampledatay31 <-train1[train1$rating == 3 ,]
sampledatay51<-train1[train1$rating > 3 ,]
sampledatay11 <-train1[train1$rating <3 ,]
glimpse(sampledatay31)


#cleaning up the text and removing special characters

fix.contractions <- function(doc) {
  doc <- gsub("won't", "will not", doc)
  doc <- gsub("can't", "can not", doc)
  doc <- gsub("n't", " not", doc)
  doc <- gsub("'ll", " will", doc)
  doc <- gsub("'re", " are", doc)
  doc <- gsub("'ve", " have", doc)
  doc <- gsub("'m", " am", doc)
  doc <- gsub("'d", " would", doc)
  doc <- gsub("<br />", "", doc)
  doc <- gsub("\n", "", doc)
  # 's could be 'is' or could be possessive: it has no expansion
  doc <- gsub("'s", "", doc)
  doc<- gsub("miss.","",doc)
  doc<- gsub("mr.","",doc)
  return(doc)
}
removeSpecialChars <- function(x) gsub("[^a-zA-Z0-9 ]", " ", x)

reduced_data <- data.table(sampledatay3)
reduced_data5 <- data.table(sampledatay5)
reduced_data1 <- data.table(sampledatay1)


reduced_data$review <- tolower(reduced_data$review)
reduced_data$review <- sapply(sampledatay3$review, removeSpecialChars)
reduced_data$review <- sapply(reduced_data$review, fix.contractions)

reduced_data5$review <- tolower(reduced_data5$review)
reduced_data5$review <- sapply(sampledatay5$review, removeSpecialChars)
reduced_data5$review <- sapply(reduced_data5$review, fix.contractions)



reduced_data1$review <- sapply(sampledatay1$review, removeSpecialChars)
reduced_data1$review <- sapply(reduced_data1$review, fix.contractions)
reduced_data1$review <- tolower(reduced_data1$review)

reviewtext<-reduced_data$review
reviewtext5<-reduced_data5$review
reviewtext1<-reduced_data1$review

glimpse(reviewtext)

revtext <- data.table(words = c(reviewtext))
revtext5 <- data.table(words = c(reviewtext5))
revtext1 <- data.table(words = c(reviewtext1))

glimpse(revtext)
summary(revtext)


set.seed(100)
alltext<-c(revtext,revtext5, revtext1)
corpus <- Corpus(VectorSource(list(alltext)))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords('english'))
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, PlainTextDocument)
tdm <- TermDocumentMatrix(corpus)
m <- as.matrix(tdm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)

wordcloud(d$word,d$freq,c(3,.51),2,100,FALSE,.1,colors = brewer.pal(8,"Dark2"))

#AFINN Lexicon 

rta <- data.table(review = c(revtext$words))
review_sentiment <- rta %>%
    unnest_tokens(word, review, token = "words") %>%
    inner_join(get_sentiments("afinn"))  %>%
    filter(!nchar(word) < 3) %>% 
    anti_join(stop_words)


glimpse(review_sentiment)
    
rta5 <- data.table(review = c(revtext5$words))
review_sentiment5 <- rta5 %>%
    unnest_tokens(word, review, token = "words") %>%
    inner_join(get_sentiments("afinn"))  %>%
    filter(!nchar(word) < 3) %>% 
    anti_join(stop_words)
    
rta1 <- data.table(review = c(revtext1$words))
review_sentiment1 <- rta1 %>%
    unnest_tokens(word, review, token = "words") %>%
    inner_join(get_sentiments("afinn"))  %>%
    filter(!nchar(word) < 3) %>% 
    anti_join(stop_words)

count<-table(review_sentiment5$value + review_sentiment$value + review_sentiment1$value)

nchar(review_sentiment1)
nchar(review_sentiment5)
nchar(review_sentiment)

barplot(count, main="Reviews", xlab="polarity of sentiments",ylab = "frequency", col = brewer.pal(11,"RdYlBu")) 


count<-table(review_sentiment5$value)

barplot(count, main="5 Star Reviews",
        xlab="polarity of sentiments",ylab = "frequency", col = brewer.pal(10,"RdYlBu")) 

count<-table(review_sentiment$value)

barplot(count, main="3 Star Reviews",
        xlab="polarity of sentiments", col = brewer.pal(10,"RdYlBu")) 

count<-table(review_sentiment1$value)

barplot(count, main="1 Star Reviews",
        xlab="polarity of sentiments", col = brewer.pal(10,"RdYlBu")) 


p_sent<-review_sentiment[review_sentiment$value>= 0,]
layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
set.seed(100)
wordcloud(p_sent$word,max.words = 100,colors=brewer.pal(6,"Dark2"))


n_sent<-review_sentiment[review_sentiment$value< 0,]
layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
set.seed(100)
wordcloud(n_sent$word,max.words = 100,colors=brewer.pal(6,"Dark2"))


p_sent5<-review_sentiment5[review_sentiment5$value>= 0,]

layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
set.seed(100)
wordcloud(p_sent5$word,max.words = 100,colors=brewer.pal(8,"Dark2"))

p_sent1<-review_sentiment1[review_sentiment1$value>= 0,]
layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
set.seed(100)
wordcloud(p_sent1$word,max.words = 100,colors=brewer.pal(8,"Dark2"))



n_sent5<-review_sentiment5[review_sentiment5$value< 0,]
layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
set.seed(100)
wordcloud(n_sent5$word,min.freq=150,colors=brewer.pal(8,"Dark2"))


n_sent1<-review_sentiment1[review_sentiment1$value< 0,]
layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
set.seed(100)
wordcloud(n_sent1$word,min.freq=100,colors=brewer.pal(6,"Paired"))

#BING Lexicon

rtb <- data.table(review = c(revtext$words))
review_sentimentb <- rta %>%
    unnest_tokens(word, review, token = "words") %>%
    inner_join(get_sentiments("bing"))
str(review_sentimentb)

countb<-table(review_sentimentb$sentiment)

barplot(countb, main="Sentiment distribution",
        ylab="Number of sentiments", col = c("red","blue"))


pos_sent<-review_sentimentb[review_sentimentb$sentiment == 'positive',]
layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
set.seed(100)

wordcloud(pos_sent$word,max.words = 100,colors=brewer.pal(8,"Dark2"))


neg_sent<-review_sentimentb[review_sentimentb$sentiment == 'negative',]
layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
set.seed(100)


wordcloud(neg_sent$word,max.words = 100,colors=brewer.pal(6,"Paired"))

#Syuzhet’s package


algotext<-gsub("http[^[:blank:]]+","",alltext)
algotext<-gsub("@\\w+","",algotext)
algotext<-gsub("[[:punct:]]"," ",algotext)
algotext<-gsub("[^[:alnum:]]"," ",algotext)
algotext<-gsub("Miss."," ",algotext)
algotext<-gsub("Mr."," ",algotext)

algosent<-get_nrc_sentiment((algotext))


algosent.positive =sum(algosent$positive)
algosent.anger =sum(algosent$anger)
algosent.anticipation =sum(algosent$anticipation)
algosent.disgust =sum(algosent$disgust)
algosent.fear =sum(algosent$fear)
algosent.joy =sum(algosent$joy)
algosent.sadness =sum(algosent$sadness)
algosent.surprise =sum(algosent$surprise)
algosent.trust =sum(algosent$trust)
algosent.negative =sum(algosent$negative)

colSums(algosent)
head(algosent)
yAxis <- c(algosent.positive,
           + algosent.anger,
           + algosent.anticipation,
           + algosent.disgust,
           + algosent.fear,
           + algosent.joy,
           + algosent.sadness,
           + algosent.surprise,
           + algosent.trust,
           + algosent.negative)
xAxis <- c("Negative","Anger","Anticipation","Disgust","Fear","Joy","Sadness","Surprise","Trust","Positive")
yRange <- range(0,yAxis) + 500
barplot(yAxis, names.arg = xAxis,
        xlab = "Emotional valence", ylab = "Score", main = "Emotional Valence", col = brewer.pal(10,"RdYlBu"))


```
```{r}
#Preprocessing Pipeline
#1. Tokenize
#2. lower casing
#3. stop word removal
#4. Stemming
#5. Adding Bigrams
#6. Transform to DFM
#7. Ensure Test and train DFM have the same features

#tokenization and cleaning



train.tokens <- tokenize_words(train$review)
train.tokens[[101]]

train.tokens <- tokenize_word_stems(train$review, stopwords = stopwords::stopwords("en"))
train.tokens[[101]]


train.tokens <- tokens(train$review,what = "word", remove_numbers = TRUE, remove_punct = TRUE, split_hyphens = TRUE, remove_symbols = TRUE)


train.tokens <- tokens_tolower(train.tokens)

train.tokens[[101]]

train.tokens<- tokens_select(train.tokens, stopwords(), selection = "remove")

train.tokens[[101]]

train.tokens<- tokens_wordstem(train.tokens, language = "english")

train.tokens[[101]]

#bag of words

train.tokens.dfm <-dfm(train.tokens, tolower = FALSE)
str(train.tokens.dfm)
train.tokens.matrix <- as.matrix(train.tokens.dfm)
view(train.tokens.matrix[1:10, 1:100])

dim(train.tokens.matrix)

colnames(train.tokens.matrix)[1:25]


train.tokens.dfm <-dfm(train.tokens, tolower = FALSE)
dim(train.tokens.dfm)
train.tokens.matrix <- as.matrix(train.tokens.dfm)

#view(train.tokens.matrix[1:10, 1:100])

#dim(train.tokens.matrix)

#colnames(train.tokens.matrix)[1:25]



#TFIDF
#term frequency
term.frequency <- function(row){
  row / sum(row)
  }

#inverse document frequency
inverse.doc.freq<- function(col){
  corpus.size<- length(col)
  doc.count<- length(which(col>0))
  log10(corpus.size /doc.count)
  
}

tf.idf <- function(tf, idf){
  tf*idf
}

#normalize documents through TF
train.tokens.df <- apply(train.tokens.matrix, 1, term.frequency)
dim(train.tokens.df)
view(train.tokens.df [1:20, 1:100])

#Calculating the Inverse Document Frequency vector
train.tokens.idf<-apply(train.tokens.matrix ,2, inverse.doc.freq)
str(train.tokens.idf)

#calculate tf-idf of our training data
train.tokens.tfidf <- apply(train.tokens.df, 2, tf.idf, idf = train.tokens.idf)
dim(train.tokens.tfidf)
view(train.tokens.tfidf [1:25, 1:25])

#transpose the matrix
train.tokens.tfidf <- t(train.tokens.tfidf)
dim(train.tokens.tfidf)
view(train.tokens.tfidf [1:25, 1:25])

#check for incomplete cases
incomplete.cases<-which(!complete.cases(train.tokens.tfidf))
train$review[incomplete.cases]

#Replace all in incomplete cases with a 0.0
train.tokens.tfidf[incomplete.cases,]<- rep(0.0, ncol(train.tokens.tfidf))
dim(train.tokens.tfidf)
sum(which(!complete.cases(train.tokens.tfidf)))


#Final tfidf data frame
train.tokens.tfidf.df <- cbind(rating = train$rating, data.frame(train.tokens.tfidf))
names(train.tokens.tfidf.df) <- make.names(names(train.tokens.tfidf.df))
view(train.tokens.tfidf.df [1:25, 1:25])


```


```{r Stratified Sample}

#stratified sample
sampledatay31 <-train1[train1$rating == 3 ,]
sampledatay51<-train1[train1$rating > 3 ,]
sampledatay11 <-train1[train1$rating <3 ,]
glimpse(sampledatay31)


#cleaning up the text and removing special characters

fix.contractions <- function(doc) {
  doc <- gsub("won't", "will not", doc)
  doc <- gsub("can't", "can not", doc)
  doc <- gsub("n't", " not", doc)
  doc <- gsub("'ll", " will", doc)
  doc <- gsub("'re", " are", doc)
  doc <- gsub("'ve", " have", doc)
  doc <- gsub("'m", " am", doc)
  doc <- gsub("'d", " would", doc)
  doc <- gsub("<br />", "", doc)
  doc <- gsub("\n", "", doc)
  # 's could be 'is' or could be possessive: it has no expansion
  doc <- gsub("'s", "", doc)
  doc<- gsub("miss.","",doc)
  doc<- gsub("mr.","",doc)
  return(doc)
}
removeSpecialChars <- function(x) gsub("[^a-zA-Z0-9 ]", " ", x)

reduced_data <- data.table(sampledatay31)
reduced_data5 <- data.table(sampledatay51)
reduced_data1 <- data.table(sampledatay11)


reduced_data$review <- tolower(reduced_data$review)
reduced_data$review <- sapply(sampledatay31$review, removeSpecialChars)
reduced_data$review <- sapply(reduced_data$review, fix.contractions)

reduced_data5$review <- tolower(reduced_data5$review)
reduced_data5$review <- sapply(sampledatay51$review, removeSpecialChars)
reduced_data5$review <- sapply(reduced_data5$review, fix.contractions)



reduced_data1$review <- sapply(sampledatay11$review, removeSpecialChars)
reduced_data1$review <- sapply(reduced_data1$review, fix.contractions)
reduced_data1$review <- tolower(reduced_data1$review)

reviewtext<-reduced_data$review
reviewtext5<-reduced_data5$review
reviewtext1<-reduced_data1$review

glimpse(reviewtext)

revtext <- data.table(words = c(reviewtext))
revtext5 <- data.table(words = c(reviewtext5))
revtext1 <- data.table(words = c(reviewtext1))

glimpse(revtext)
summary(revtext)


set.seed(100)
alltext<-c(revtext,revtext5, revtext1)
corpus <- Corpus(VectorSource(list(alltext)))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords('english'))
corpus <- tm_map(corpus, stemDocument)
tdm <- TermDocumentMatrix(corpus)
m <- as.matrix(tdm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)

wordcloud(d$word,d$freq,c(3,.51),2,100,FALSE,.1,colors = brewer.pal(8,"Dark2"))

#AFINN Lexicon 

rta <- data.table(review = c(revtext$words))
review_sentiment <- rta %>%
    unnest_tokens(word, review, token = "words") %>%
    inner_join(get_sentiments("afinn"))  %>%
    filter(!nchar(word) < 3) %>% 
    anti_join(stop_words)


glimpse(review_sentiment)
    
rta5 <- data.table(review = c(revtext5$words))
review_sentiment5 <- rta5 %>%
    unnest_tokens(word, review, token = "words") %>%
    inner_join(get_sentiments("afinn"))  %>%
    filter(!nchar(word) < 3) %>% 
    anti_join(stop_words)
    
rta1 <- data.table(review = c(revtext1$words))
review_sentiment1 <- rta1 %>%
    unnest_tokens(word, review, token = "words") %>%
    inner_join(get_sentiments("afinn"))  %>%
    filter(!nchar(word) < 3) %>% 
    anti_join(stop_words)

count<-table(review_sentiment5$value + review_sentiment$value + review_sentiment1$value)

nchar(review_sentiment1)
nchar(review_sentiment5)
nchar(review_sentiment)

barplot(count, main="Reviews", xlab="polarity of sentiments",ylab = "frequency", col = brewer.pal(11,"RdYlBu")) 


count<-table(review_sentiment5$value)

barplot(count, main="5 Star Reviews",
        xlab="polarity of sentiments",ylab = "frequency", col = brewer.pal(10,"RdYlBu")) 

count<-table(review_sentiment$value)

barplot(count, main="3 Star Reviews",
        xlab="polarity of sentiments", col = brewer.pal(10,"RdYlBu")) 

count<-table(review_sentiment1$value)

barplot(count, main="1 Star Reviews",
        xlab="polarity of sentiments", col = brewer.pal(10,"RdYlBu")) 


p_sent<-review_sentiment[review_sentiment$value>= 0,]
layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
set.seed(100)
wordcloud(p_sent$word,max.words = 100,colors=brewer.pal(6,"Dark2"))


n_sent<-review_sentiment[review_sentiment$value< 0,]
layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
set.seed(100)
wordcloud(n_sent$word,max.words = 100,colors=brewer.pal(6,"Dark2"))


p_sent5<-review_sentiment5[review_sentiment5$value>= 0,]

layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
set.seed(100)
wordcloud(p_sent5$word,max.words = 100,colors=brewer.pal(8,"Dark2"))

p_sent1<-review_sentiment1[review_sentiment1$value>= 0,]
layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
set.seed(100)
wordcloud(p_sent1$word,max.words = 100,colors=brewer.pal(8,"Dark2"))



n_sent5<-review_sentiment5[review_sentiment5$value< 0,]
layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
set.seed(100)
wordcloud(n_sent5$word,min.freq=150,colors=brewer.pal(8,"Dark2"))


n_sent1<-review_sentiment1[review_sentiment1$value< 0,]
layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
set.seed(100)
wordcloud(n_sent1$word,min.freq=100,colors=brewer.pal(6,"Paired"))

#BING Lexicon

rtb <- data.table(review = c(revtext$words))
review_sentimentb <- rta %>%
    unnest_tokens(word, review, token = "words") %>%
    inner_join(get_sentiments("bing"))
str(review_sentimentb)

countb<-table(review_sentimentb$sentiment)

barplot(countb, main="Sentiment distribution",
        ylab="Number of sentiments", col = c("red","blue"))


pos_sent<-review_sentimentb[review_sentimentb$sentiment == 'positive',]
layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
set.seed(100)

wordcloud(pos_sent$word,max.words = 100,colors=brewer.pal(8,"Dark2"))


neg_sent<-review_sentimentb[review_sentimentb$sentiment == 'negative',]
layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
set.seed(100)


wordcloud(neg_sent$word,max.words = 100,colors=brewer.pal(6,"Paired"))

#Syuzhet’s package


algotext<-gsub("http[^[:blank:]]+","",alltext)
algotext<-gsub("@\\w+","",algotext)
algotext<-gsub("[[:punct:]]"," ",algotext)
algotext<-gsub("[^[:alnum:]]"," ",algotext)
algotext<-gsub("Miss."," ",algotext)
algotext<-gsub("Mr."," ",algotext)

algosent<-get_nrc_sentiment((algotext))


algosent.positive =sum(algosent$positive)
algosent.anger =sum(algosent$anger)
algosent.anticipation =sum(algosent$anticipation)
algosent.disgust =sum(algosent$disgust)
algosent.fear =sum(algosent$fear)
algosent.joy =sum(algosent$joy)
algosent.sadness =sum(algosent$sadness)
algosent.surprise =sum(algosent$surprise)
algosent.trust =sum(algosent$trust)
algosent.negative =sum(algosent$negative)

colSums(algosent)
head(algosent)
yAxis <- c(algosent.positive,
           + algosent.anger,
           + algosent.anticipation,
           + algosent.disgust,
           + algosent.fear,
           + algosent.joy,
           + algosent.sadness,
           + algosent.surprise,
           + algosent.trust,
           + algosent.negative)
xAxis <- c("Negative","Anger","Anticipation","Disgust","Fear","Joy","Sadness","Surprise","Trust","Positive")
yRange <- range(0,yAxis) + 500
barplot(yAxis, names.arg = xAxis,
        xlab = "Emotional valence", ylab = "Score", main = "Emotional Valence", col = brewer.pal(10,"RdYlBu"))

```

```{r}
#Preprocessing Pipeline
#1. Tokenize
#2. lower casing
#3. stop word removal
#4. Stemming
#5. Adding Bigrams
#6. Transform to DFM
#7. Ensure Test and train DFM have the same features

#tokenization and cleaning


glimpse(train)
train.tokens <- tokenize_words(train$review)
train.tokens[[101]]

train.tokens <- tokenize_word_stems(train$review, stopwords = stopwords::stopwords("en"))
train.tokens[[101]]


train.tokens <- tokens(train$review,what = "word", remove_numbers = TRUE, remove_punct = TRUE, split_hyphens = TRUE, remove_symbols = TRUE)


train.tokens <- tokens_tolower(train.tokens)

train.tokens[[101]]

train.tokens<- tokens_select(train.tokens, stopwords(), selection = "remove")

train.tokens[[101]]

train.tokens<- tokens_wordstem(train.tokens, language = "english")

train.tokens[[101]]

#bag of words

train.tokens.dfm <-dfm(train.tokens, tolower = FALSE)
str(train.tokens.dfm)
train.tokens.matrix <- as.matrix(train.tokens.dfm)

#view(train.tokens.matrix[1:10, 1:100])

dim(train.tokens.matrix)

colnames(train.tokens.matrix)[1:25]


train.tokens.dfm <-dfm(train.tokens, tolower = FALSE)
#dim(train.tokens.dfm)
train.tokens.matrix <- as.matrix(train.tokens.dfm)

#view(train.tokens.matrix[1:10, 1:100])

#dim(train.tokens.matrix)

colnames(train.tokens.matrix)[1:25]

#Cross Validation
train.tokens.df <-cbind(rating = train$rating, convert(train.tokens.dfm, to = "data.frame"))

#clean column names. 
names(train.tokens.df) <- make.names(names(train.tokens.df))

```


```{r Rpart and SVM}
#Cross Validation
train.tokens.df <-cbind(rating = train$rating, convert(train.tokens.dfm, to = "data.frame"))

#clean column names. 
names(train.tokens.df) <- make.names(names(train.tokens.df))
# drops <- c("document")
# train.tokens.df <- train.tokens.df[ , !(names(train.tokens.df) %in% drops)]

# use caret to create stratified(because the data is not balanced) folds for 10-fold cross validation repeated 3 times
set.seed(33445)
cv.folds<-createMultiFolds(train$rating, k = 10, times = 3)

cv.cntrl<- trainControl(method = "repeatedcv", number = 10, repeats = 3, index = cv.folds)

#timing the code execution
start.time <- Sys.time()

#make a cluster to work on 8 logical cores
cl<-makeCluster(8, type = "SOCK")
registerDoSNOW(cl)

# drops <- c("document")
# train.tokens.df <- train.tokens.df[ , !(names(train.tokens.df) %in% drops)]


rpart.cv.1 <- train(rating ~ ., data = train.tokens.df, method = "rpart", trControl = cv.cntrl, tuneLength = 7)
svmLinear3.cv.1<- train(rating ~., data = train.tokens.df, method = "svmLinear3", trControl = cv.cntrl, tuneLength = 7)


stopCluster(cl)

#Execution time
total.time<- Sys.time() - start.time
total.time

svmLinear3.cv.1
rpart.cv.1

```

#TFIDF
#term frequency
term.frequency <- function(row){
  row / sum(row)
  }

#inverse document frequency
inverse.doc.freq<- function(col){
  corpus.size<- length(col)
  doc.count<- length(which(col>0))
  log10(corpus.size /doc.count)
  
}

tf.idf <- function(tf, idf){
  tf*idf
}

#normalize documents through TF
train.tokens.df <- apply(train.tokens.matrix, 1, term.frequency)
dim(train.tokens.df)
view(train.tokens.df [1:20, 1:100])

#Calculating the Inverse Document Frequency vector
train.tokens.idf<-apply(train.tokens.matrix ,2, inverse.doc.freq)
str(train.tokens.idf)

#calculate tf-idf of our training data
train.tokens.tfidf <- apply(train.tokens.df, 2, tf.idf, idf = train.tokens.idf)
dim(train.tokens.tfidf)
view(train.tokens.tfidf [1:25, 1:25])

#transpose the matrix
train.tokens.tfidf <- t(train.tokens.tfidf)
dim(train.tokens.tfidf)
view(train.tokens.tfidf [1:25, 1:25])

#check for incomplete cases
incomplete.cases<-which(!complete.cases(train.tokens.tfidf))
train$review[incomplete.cases]

#Replace all in incomplete cases with a 0.0
train.tokens.tfidf[incomplete.cases,]<- rep(0.0, ncol(train.tokens.tfidf))
dim(train.tokens.tfidf)
sum(which(!complete.cases(train.tokens.tfidf)))


#Final tfidf data frame
train.tokens.tfidf.df <- cbind(rating = train$rating, data.frame(train.tokens.tfidf))
names(train.tokens.tfidf.df) <- make.names(names(train.tokens.tfidf.df))
view(train.tokens.tfidf.df [1:25, 1:25])


```




```{r CART and SVM}
#Cross Validation
train.tokens.df <-cbind(rating = train$rating, convert(train.tokens.dfm, to = 'data.frame'))

#clean column names. 
names(train.tokens.df) <- make.names(names(train.tokens.df))
#drops <- c('document')
#train.tokens.df <- train.tokens.df[ , !(names(train.tokens.df) %in% drops)]

# use caret to create stratified(because the data is not balanced) folds for 10-fold cross validation repeated 3 times
set.seed(33445)
cv.folds<-createMultiFolds(train$rating, k = 10, times = 3)

cv.cntrl<- trainControl(method = 'repeatedcv', number = 10, repeats = 3, index = cv.folds)

#timing the code execution
start.time <- Sys.time()

#make a cluster to work on 8 logical cores
cl<-makeCluster(8, type = 'SOCK')
registerDoSNOW(cl)
#drops <- c('document')
#train.tokens.df <- train.tokens.df[ , !(names(train.tokens.df) %in% drops)]



rpart.cv.1 <- train(rating ~ ., data = train.tokens.df, method = 'rpart', trControl = cv.cntrl, tuneLength = 7)
svmLinear3.cv.1<- train(rating ~., data = train.tokens.df, method = 'svmLinear3', trControl = cv.cntrl, tuneLength = 7)


stopCluster(cl)

#Execution time
total.time<- Sys.time() - start.time
total.time

svmLinear3.cv.1
rpart.cv.1

```





```{r TFIDF Transformation}
#TFIDF
#term frequency
term.frequency <- function(row){
  row / sum(row)
  }

#inverse document frequency
inverse.doc.freq<- function(col){
  corpus.size<- length(col)
  doc.count<- length(which(col>0))
  log10(corpus.size /doc.count)
  
}

tf.idf <- function(tf, idf){
  tf*idf
}

#normalize documents through TF
train.tokens.df <- apply(train.tokens.matrix, 1, term.frequency)
dim(train.tokens.df)
view(train.tokens.df [1:20, 1:100])

#Calculating the Inverse Document Frequency vector
train.tokens.idf<-apply(train.tokens.matrix ,2, inverse.doc.freq)
str(train.tokens.idf)

#calculate tf-idf of our training data
train.tokens.tfidf <- apply(train.tokens.df, 2, tf.idf, idf = train.tokens.idf)
dim(train.tokens.tfidf)
view(train.tokens.tfidf [1:25, 1:25])

#transpose the matrix
train.tokens.tfidf <- t(train.tokens.tfidf)
dim(train.tokens.tfidf)
view(train.tokens.tfidf [1:25, 1:25])

#check for incomplete cases
incomplete.cases<-which(!complete.cases(train.tokens.tfidf))
train$review[incomplete.cases]

#Replace all in incomplete cases with a 0.0
train.tokens.tfidf[incomplete.cases,]<- rep(0.0, ncol(train.tokens.tfidf))
dim(train.tokens.tfidf)
sum(which(!complete.cases(train.tokens.tfidf)))


#Final tfidf data frame
train.tokens.tfidf.df <- cbind(rating = train$rating, data.frame(train.tokens.tfidf))
names(train.tokens.tfidf.df) <- make.names(names(train.tokens.tfidf.df))
view(train.tokens.tfidf.df [1:25, 1:25])
```
```{r Post IDF Rpart & SVD}

set.seed(33445)
cv.folds<-createMultiFolds(train$rating, k = 10, times = 3)

cv.cntrl<- trainControl(method = 'repeatedcv', number = 10, repeats = 3, index = cv.folds)

#timing the code execution
start.time <- Sys.time()

# make a cluster to work on 8 logical cores
cl<-makeCluster(8, type = 'SOCK')
registerDoSNOW(cl)

drops <- c('document')
train.tokens.df <- train.tokens.df[ , !(names(train.tokens.df) %in% drops)]


rpart.cv.2<- train(rating ~ ., data = train.tokens.tfidf.df, method = 'rpart', trControl = cv.cntrl, tuneLength = 7)
svmlinear3.cv.2<- train(rating ~., data = train.tokens.tfidf.df, method = 'svmlinear3', trControl = cv.cntrl, tuneLength = 7)


stopCluster(cl)

#Execution time
total.time2<- Sys.time() - start.time
total.time2

rpart.cv.1
rpart.cv.2
svmlinear3.cv.11
svmlinear3.cv.2

```
```{r}
#Cross Validation
gc()
train.tokens.df <-cbind(rating = train$rating, convert(train.tokens.dfm, to = "data.frame"))

#clean column names. 
names(train.tokens.df) <- make.names(names(train.tokens.df))


#use caret to create stratified(because the data is not balanced) folds for 10-fold cross validation repeated 3 times


#install.packages("caret", dependencies = TRUE)
#library(caret)

set.seed(33445)
cv.folds<-createMultiFolds(train$rating, k = 10, times = 3)

cv.cntrl<- trainControl(method = "repeatedcv", number = 10, repeats = 3, index = cv.folds)

#timing the code execution
start.time <- Sys.time()

#make a cluster to work on 7 logical cores
cl<-makeCluster(7, type = "SOCK")
registerDoSNOW(cl)

drops <- c("document")
train.tokens.df <- train.tokens.df[ , !(names(train.tokens.df) %in% drops)]


rpart.cv.1 <- train(rating ~ ., data = train.tokens.df, method = "rpart", trControl = cv.cntrl, tuneLength = 12)


stopCluster(cl)

#Execution time
total.time<- Sys.time() - start.time
total.time

#orfsvm.cv.1

plot(rpart.cv.1)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
